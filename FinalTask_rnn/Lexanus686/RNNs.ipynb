{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinalTask.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGh_PklSqeUT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip3 -qq install torch\n",
        "#!pip3 -qq install bokeh==0.13.0\n",
        "#!pip3 -qq install gensim==3.6.0\n",
        "#!pip3 -qq install nltk\n",
        "#!pip3 -qq install scikit-learn==0.20.2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yx9jxSocrZa2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    from torch.cuda import FloatTensor, LongTensor\n",
        "else:\n",
        "    from torch import FloatTensor, LongTensor\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6xQD1sA6VEo",
        "colab_type": "code",
        "outputId": "e6d6799a-5904-4623-89cf-45f5a4cac8d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjje9vUyswE8",
        "colab_type": "code",
        "outputId": "57c3d708-b72d-4b13-cf1a-961ea0468812",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDd4kCPHs3MV",
        "colab_type": "code",
        "outputId": "1e30c64b-b3e0-40e9-f03a-320ebb75e112",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        }
      },
      "source": [
        "for word, tag in data[0]:\n",
        "    print('{:15}\\t{}'.format(word, tag))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The            \tDET\n",
            "Fulton         \tNOUN\n",
            "County         \tNOUN\n",
            "Grand          \tADJ\n",
            "Jury           \tNOUN\n",
            "said           \tVERB\n",
            "Friday         \tNOUN\n",
            "an             \tDET\n",
            "investigation  \tNOUN\n",
            "of             \tADP\n",
            "Atlanta's      \tNOUN\n",
            "recent         \tADJ\n",
            "primary        \tNOUN\n",
            "election       \tNOUN\n",
            "produced       \tVERB\n",
            "``             \t.\n",
            "no             \tDET\n",
            "evidence       \tNOUN\n",
            "''             \t.\n",
            "that           \tADP\n",
            "any            \tDET\n",
            "irregularities \tNOUN\n",
            "took           \tVERB\n",
            "place          \tNOUN\n",
            ".              \t.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5eGVR2qs7KZ",
        "colab_type": "code",
        "outputId": "7ce43956-99c9-4c25-c297-40757560b0cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
        "\n",
        "print('Words count in train set:', sum(len(sent) for sent in train_data))\n",
        "print('Words count in val set:', sum(len(sent) for sent in val_data))\n",
        "print('Words count in test set:', sum(len(sent) for sent in test_data))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words count in train set: 739769\n",
            "Words count in val set: 130954\n",
            "Words count in test set: 290469\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXafdv-6uUUN",
        "colab_type": "code",
        "outputId": "bea41dcb-cb5d-4df3-f1f2-8f9f06e4a3d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "words = {word for sample in train_data for word, tag in sample}\n",
        "word2ind = {word: ind + 1 for ind, word in enumerate(words)}\n",
        "word2ind['<pad>'] = 0\n",
        "\n",
        "tags = {tag for sample in train_data for word, tag in sample}\n",
        "tag2ind = {tag: ind + 1 for ind, tag in enumerate(tags)}\n",
        "tag2ind['<pad>'] = 0\n",
        "\n",
        "print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique words in train = 45441. Tags = {'PRON', 'VERB', '.', 'ADP', 'ADV', 'DET', 'NUM', 'CONJ', 'X', 'PRT', 'NOUN', 'ADJ'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YipHcZLzu3t0",
        "colab_type": "code",
        "outputId": "6887669b-fe16-4099-b78d-9b2d17430ac2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n",
        "tag_distribution = [tag_distribution[tag] for tag in tags]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "bar_width = 0.35\n",
        "plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n",
        "plt.xticks(np.arange(len(tags)), tags)\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmkAAAEvCAYAAAAemFY+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LND23RCvSyG",
        "colab_type": "code",
        "outputId": "4cdadd8a-8da9-4305-b092-a65459d704b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import nltk\n",
        "\n",
        "default_tagger = nltk.DefaultTagger('NN')\n",
        "\n",
        "unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n",
        "print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of unigram tagger = 92.62%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yetEASsvYS4",
        "colab_type": "code",
        "outputId": "8ad554ad-9f09-47d2-f460-4451f6b1d93d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n",
        "print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of bigram tagger = 93.42%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMWR5SdTvbBT",
        "colab_type": "code",
        "outputId": "cb15f052-8f1b-40dd-a34f-ff6c19a39001",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "trigram_tagger = nltk.TrigramTagger(train_data)\n",
        "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of trigram tagger = 23.33%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsVkdFkavhKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_data(data, word2ind, tag2ind):\n",
        "    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n",
        "    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n",
        "X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n",
        "X_test, y_test = convert_data(test_data, word2ind, tag2ind)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQYYopZJv6_P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def iterate_batches(data, batch_size):\n",
        "    X, y = data\n",
        "    n_samples = len(X)\n",
        "\n",
        "    indices = np.arange(n_samples)\n",
        "    np.random.shuffle(indices)\n",
        "    \n",
        "    for start in range(0, n_samples, batch_size):\n",
        "        end = min(start + batch_size, n_samples)\n",
        "        \n",
        "        batch_indices = indices[start:end]\n",
        "        \n",
        "        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n",
        "        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        \n",
        "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
        "            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n",
        "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
        "            \n",
        "        yield X_batch, y_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMA6NuWRv9Og",
        "colab_type": "code",
        "outputId": "2e249466-6fa1-4c00-d582-7684be0df139",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n",
        "\n",
        "X_batch.shape, y_batch.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((32, 4), (32, 4))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrUGe-giv_o0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "        super(LSTMTagger, self).__init__()\n",
        "\n",
        "        self.lstm_hidden_dim = lstm_hidden_dim\n",
        "\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, word_emb_dim)\n",
        "\n",
        "        self.lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, num_layers=lstm_layers_count)\n",
        "        self.hidden2tag = nn.Linear(lstm_hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.word_embeddings(inputs)\n",
        "        lstm_out, _ = self.lstm(embeds)\n",
        "        tag_space = self.hidden2tag(lstm_out)\n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        return tag_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rxm6U-K6zvmu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ")\n",
        "\n",
        "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
        "\n",
        "logits = model(X_batch)\n",
        "\n",
        "def calc_accuracy(logits, y_batch):\n",
        "    outs = torch.argmax(logits, dim=-1)\n",
        "    reals = ((outs == y_batch).float() * (y_batch != 0).float()).sum().item()\n",
        "    totals = (y_batch != 0).float().sum().item()\n",
        "    return reals, totals"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIBfxndAzxET",
        "colab_type": "code",
        "outputId": "d22e8ae7-f928-49a3-a2bc-698665c8d2b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion(logits.view(-1, logits.shape[-1]), y_batch.view(-1)).item()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.5506138801574707"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWHkPNvg4dUk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
        "    epoch_loss = 0\n",
        "    correct_count = 0\n",
        "    sum_count = 0\n",
        "    \n",
        "    is_train = not optimizer is None\n",
        "    name = name or ''\n",
        "    model.train(is_train)\n",
        "    \n",
        "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
        "    \n",
        "    with torch.autograd.set_grad_enabled(is_train):\n",
        "        with tqdm(total=batches_count) as progress_bar:\n",
        "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
        "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "                logits = model(X_batch)\n",
        "\n",
        "                loss = criterion(logits.view(-1, logits.shape[-1]), y_batch.view(-1))\n",
        "                \n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                if optimizer:\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                cur_correct_count, cur_sum_count = calc_accuracy(logits, y_batch)\n",
        "\n",
        "                correct_count += cur_correct_count\n",
        "                sum_count += cur_sum_count\n",
        "\n",
        "                progress_bar.update()\n",
        "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
        "                )\n",
        "                \n",
        "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
        "            )\n",
        "\n",
        "    return epoch_loss / batches_count, correct_count / sum_count\n",
        "\n",
        "\n",
        "def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n",
        "        val_data=None, val_batch_size=None):\n",
        "        \n",
        "    if not val_data is None and val_batch_size is None:\n",
        "        val_batch_size = batch_size\n",
        "        \n",
        "    for epoch in range(epochs_count):\n",
        "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
        "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
        "        \n",
        "        if not val_data is None:\n",
        "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPzLCjnn4yj-",
        "colab_type": "code",
        "outputId": "1eab8b9a-69cf-4872-ecb0-6b0dc4e9314a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 5] Train: Loss = 0.46537, Accuracy = 68.70%: 100%|██████████| 572/572 [00:05<00:00, 95.65it/s]\n",
            "[1 / 5]   Val: Loss = 0.16854, Accuracy = 81.19%: 100%|██████████| 13/13 [00:00<00:00, 65.95it/s]\n",
            "[2 / 5] Train: Loss = 0.16626, Accuracy = 84.81%: 100%|██████████| 572/572 [00:05<00:00, 98.47it/s]\n",
            "[2 / 5]   Val: Loss = 0.13500, Accuracy = 86.61%: 100%|██████████| 13/13 [00:00<00:00, 64.25it/s]\n",
            "[3 / 5] Train: Loss = 0.12703, Accuracy = 88.25%: 100%|██████████| 572/572 [00:05<00:00, 96.23it/s]\n",
            "[3 / 5]   Val: Loss = 0.13560, Accuracy = 88.67%: 100%|██████████| 13/13 [00:00<00:00, 65.58it/s]\n",
            "[4 / 5] Train: Loss = 0.10661, Accuracy = 89.81%: 100%|██████████| 572/572 [00:05<00:00, 96.20it/s]\n",
            "[4 / 5]   Val: Loss = 0.14122, Accuracy = 89.87%: 100%|██████████| 13/13 [00:00<00:00, 68.45it/s]\n",
            "[5 / 5] Train: Loss = 0.09473, Accuracy = 90.89%: 100%|██████████| 572/572 [00:05<00:00, 97.59it/s]\n",
            "[5 / 5]   Val: Loss = 0.14944, Accuracy = 90.45%: 100%|██████████| 13/13 [00:00<00:00, 67.16it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_Ob7TGtHSE3",
        "colab_type": "code",
        "outputId": "9776e9f7-5c9a-4cd1-a3ea-1311b955a9b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "for X_batch, y_batch in iterate_batches((X_test, y_test), 64):\n",
        "    X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "    logits = model(X_batch)\n",
        "    mask = (y_batch != 0).float()\n",
        "    preds = torch.argmax(logits, dim=-1)\n",
        "    correct += ((preds == y_batch).float() * mask).sum()\n",
        "    total += mask.sum()\n",
        "\n",
        "print(correct/total)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.8785, device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqPEbnWFDrRO",
        "colab_type": "code",
        "outputId": "0d82c1de-fbc8-492b-8b47-adf18f48f54a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "class BidirectionalLSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.word_embeddings = nn.Embedding(vocab_size, word_emb_dim)\n",
        "        self.lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, num_layers=lstm_layers_count, bidirectional=True)\n",
        "        self.hidden_to_tag = nn.Linear(2 * lstm_hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeddings = self.word_embeddings(inputs)\n",
        "        lstm_out, _ = self.lstm(embeddings)\n",
        "        tag_space = self.hidden_to_tag(lstm_out)\n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        return tag_scores\n",
        "\n",
        "model = BidirectionalLSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 50] Train: Loss = 0.59500, Accuracy = 81.68%: 100%|██████████| 572/572 [00:07<00:00, 78.13it/s]\n",
            "[1 / 50]   Val: Loss = 0.29673, Accuracy = 90.31%: 100%|██████████| 13/13 [00:00<00:00, 48.14it/s]\n",
            "[2 / 50] Train: Loss = 0.22864, Accuracy = 92.80%: 100%|██████████| 572/572 [00:07<00:00, 78.16it/s]\n",
            "[2 / 50]   Val: Loss = 0.20340, Accuracy = 93.26%: 100%|██████████| 13/13 [00:00<00:00, 48.27it/s]\n",
            "[3 / 50] Train: Loss = 0.15022, Accuracy = 95.39%: 100%|██████████| 572/572 [00:07<00:00, 78.88it/s]\n",
            "[3 / 50]   Val: Loss = 0.15706, Accuracy = 94.88%: 100%|██████████| 13/13 [00:00<00:00, 49.32it/s]\n",
            "[4 / 50] Train: Loss = 0.10881, Accuracy = 96.71%: 100%|██████████| 572/572 [00:07<00:00, 77.91it/s]\n",
            "[4 / 50]   Val: Loss = 0.14094, Accuracy = 95.32%: 100%|██████████| 13/13 [00:00<00:00, 47.67it/s]\n",
            "[5 / 50] Train: Loss = 0.07961, Accuracy = 97.59%: 100%|██████████| 572/572 [00:07<00:00, 78.64it/s]\n",
            "[5 / 50]   Val: Loss = 0.13530, Accuracy = 95.49%: 100%|██████████| 13/13 [00:00<00:00, 50.13it/s]\n",
            "[6 / 50] Train: Loss = 0.05811, Accuracy = 98.26%: 100%|██████████| 572/572 [00:07<00:00, 79.75it/s]\n",
            "[6 / 50]   Val: Loss = 0.12981, Accuracy = 95.68%: 100%|██████████| 13/13 [00:00<00:00, 51.33it/s]\n",
            "[7 / 50] Train: Loss = 0.04323, Accuracy = 98.73%: 100%|██████████| 572/572 [00:07<00:00, 79.21it/s]\n",
            "[7 / 50]   Val: Loss = 0.12926, Accuracy = 95.80%: 100%|██████████| 13/13 [00:00<00:00, 48.48it/s]\n",
            "[8 / 50] Train: Loss = 0.03295, Accuracy = 99.06%: 100%|██████████| 572/572 [00:07<00:00, 78.97it/s]\n",
            "[8 / 50]   Val: Loss = 0.12862, Accuracy = 95.92%: 100%|██████████| 13/13 [00:00<00:00, 48.88it/s]\n",
            "[9 / 50] Train: Loss = 0.02544, Accuracy = 99.28%: 100%|██████████| 572/572 [00:07<00:00, 78.45it/s]\n",
            "[9 / 50]   Val: Loss = 0.13908, Accuracy = 95.67%: 100%|██████████| 13/13 [00:00<00:00, 51.21it/s]\n",
            "[10 / 50] Train: Loss = 0.01911, Accuracy = 99.46%: 100%|██████████| 572/572 [00:07<00:00, 79.81it/s]\n",
            "[10 / 50]   Val: Loss = 0.14222, Accuracy = 95.76%: 100%|██████████| 13/13 [00:00<00:00, 50.40it/s]\n",
            "[11 / 50] Train: Loss = 0.01499, Accuracy = 99.59%: 100%|██████████| 572/572 [00:07<00:00, 79.43it/s]\n",
            "[11 / 50]   Val: Loss = 0.14493, Accuracy = 95.74%: 100%|██████████| 13/13 [00:00<00:00, 51.58it/s]\n",
            "[12 / 50] Train: Loss = 0.01253, Accuracy = 99.65%: 100%|██████████| 572/572 [00:07<00:00, 79.01it/s]\n",
            "[12 / 50]   Val: Loss = 0.14341, Accuracy = 95.97%: 100%|██████████| 13/13 [00:00<00:00, 46.84it/s]\n",
            "[13 / 50] Train: Loss = 0.00971, Accuracy = 99.72%: 100%|██████████| 572/572 [00:07<00:00, 79.27it/s]\n",
            "[13 / 50]   Val: Loss = 0.16076, Accuracy = 95.67%: 100%|██████████| 13/13 [00:00<00:00, 52.77it/s]\n",
            "[14 / 50] Train: Loss = 0.00846, Accuracy = 99.75%: 100%|██████████| 572/572 [00:07<00:00, 79.71it/s]\n",
            "[14 / 50]   Val: Loss = 0.15835, Accuracy = 95.88%: 100%|██████████| 13/13 [00:00<00:00, 50.34it/s]\n",
            "[15 / 50] Train: Loss = 0.00838, Accuracy = 99.74%: 100%|██████████| 572/572 [00:07<00:00, 78.89it/s]\n",
            "[15 / 50]   Val: Loss = 0.16761, Accuracy = 95.70%: 100%|██████████| 13/13 [00:00<00:00, 51.45it/s]\n",
            "[16 / 50] Train: Loss = 0.00755, Accuracy = 99.76%: 100%|██████████| 572/572 [00:07<00:00, 79.02it/s]\n",
            "[16 / 50]   Val: Loss = 0.18034, Accuracy = 95.60%: 100%|██████████| 13/13 [00:00<00:00, 49.56it/s]\n",
            "[17 / 50] Train: Loss = 0.00761, Accuracy = 99.76%: 100%|██████████| 572/572 [00:07<00:00, 79.00it/s]\n",
            "[17 / 50]   Val: Loss = 0.17988, Accuracy = 95.65%: 100%|██████████| 13/13 [00:00<00:00, 51.20it/s]\n",
            "[18 / 50] Train: Loss = 0.00738, Accuracy = 99.75%: 100%|██████████| 572/572 [00:07<00:00, 80.62it/s]\n",
            "[18 / 50]   Val: Loss = 0.18330, Accuracy = 95.67%: 100%|██████████| 13/13 [00:00<00:00, 49.72it/s]\n",
            "[19 / 50] Train: Loss = 0.00572, Accuracy = 99.80%: 100%|██████████| 572/572 [00:07<00:00, 79.82it/s]\n",
            "[19 / 50]   Val: Loss = 0.18737, Accuracy = 95.66%: 100%|██████████| 13/13 [00:00<00:00, 50.43it/s]\n",
            "[20 / 50] Train: Loss = 0.00663, Accuracy = 99.78%: 100%|██████████| 572/572 [00:07<00:00, 79.60it/s]\n",
            "[20 / 50]   Val: Loss = 0.18994, Accuracy = 95.65%: 100%|██████████| 13/13 [00:00<00:00, 48.67it/s]\n",
            "[21 / 50] Train: Loss = 0.00613, Accuracy = 99.78%: 100%|██████████| 572/572 [00:07<00:00, 79.72it/s]\n",
            "[21 / 50]   Val: Loss = 0.19365, Accuracy = 95.61%: 100%|██████████| 13/13 [00:00<00:00, 51.38it/s]\n",
            "[22 / 50] Train: Loss = 0.00593, Accuracy = 99.78%: 100%|██████████| 572/572 [00:07<00:00, 80.85it/s]\n",
            "[22 / 50]   Val: Loss = 0.20310, Accuracy = 95.57%: 100%|██████████| 13/13 [00:00<00:00, 48.00it/s]\n",
            "[23 / 50] Train: Loss = 0.00501, Accuracy = 99.81%: 100%|██████████| 572/572 [00:07<00:00, 79.59it/s]\n",
            "[23 / 50]   Val: Loss = 0.19599, Accuracy = 95.69%: 100%|██████████| 13/13 [00:00<00:00, 50.08it/s]\n",
            "[24 / 50] Train: Loss = 0.00596, Accuracy = 99.79%: 100%|██████████| 572/572 [00:07<00:00, 79.94it/s]\n",
            "[24 / 50]   Val: Loss = 0.20433, Accuracy = 95.68%: 100%|██████████| 13/13 [00:00<00:00, 50.15it/s]\n",
            "[25 / 50] Train: Loss = 0.00628, Accuracy = 99.77%: 100%|██████████| 572/572 [00:07<00:00, 80.03it/s]\n",
            "[25 / 50]   Val: Loss = 0.20286, Accuracy = 95.63%: 100%|██████████| 13/13 [00:00<00:00, 50.19it/s]\n",
            "[26 / 50] Train: Loss = 0.00517, Accuracy = 99.80%: 100%|██████████| 572/572 [00:07<00:00, 81.22it/s]\n",
            "[26 / 50]   Val: Loss = 0.20550, Accuracy = 95.62%: 100%|██████████| 13/13 [00:00<00:00, 51.56it/s]\n",
            "[27 / 50] Train: Loss = 0.00515, Accuracy = 99.82%: 100%|██████████| 572/572 [00:07<00:00, 79.81it/s]\n",
            "[27 / 50]   Val: Loss = 0.20489, Accuracy = 95.73%: 100%|██████████| 13/13 [00:00<00:00, 48.28it/s]\n",
            "[28 / 50] Train: Loss = 0.00562, Accuracy = 99.80%: 100%|██████████| 572/572 [00:07<00:00, 80.05it/s]\n",
            "[28 / 50]   Val: Loss = 0.20586, Accuracy = 95.68%: 100%|██████████| 13/13 [00:00<00:00, 48.26it/s]\n",
            "[29 / 50] Train: Loss = 0.00581, Accuracy = 99.80%: 100%|██████████| 572/572 [00:07<00:00, 79.33it/s]\n",
            "[29 / 50]   Val: Loss = 0.20118, Accuracy = 95.77%: 100%|██████████| 13/13 [00:00<00:00, 50.58it/s]\n",
            "[30 / 50] Train: Loss = 0.00605, Accuracy = 99.80%: 100%|██████████| 572/572 [00:07<00:00, 80.76it/s]\n",
            "[30 / 50]   Val: Loss = 0.21665, Accuracy = 95.61%: 100%|██████████| 13/13 [00:00<00:00, 49.11it/s]\n",
            "[31 / 50] Train: Loss = 0.00578, Accuracy = 99.79%: 100%|██████████| 572/572 [00:07<00:00, 80.42it/s]\n",
            "[31 / 50]   Val: Loss = 0.21868, Accuracy = 95.62%: 100%|██████████| 13/13 [00:00<00:00, 53.13it/s]\n",
            "[32 / 50] Train: Loss = 0.00517, Accuracy = 99.81%: 100%|██████████| 572/572 [00:07<00:00, 78.90it/s]\n",
            "[32 / 50]   Val: Loss = 0.21299, Accuracy = 95.72%: 100%|██████████| 13/13 [00:00<00:00, 51.47it/s]\n",
            "[33 / 50] Train: Loss = 0.00465, Accuracy = 99.81%: 100%|██████████| 572/572 [00:07<00:00, 79.92it/s]\n",
            "[33 / 50]   Val: Loss = 0.21782, Accuracy = 95.64%: 100%|██████████| 13/13 [00:00<00:00, 49.70it/s]\n",
            "[34 / 50] Train: Loss = 0.00461, Accuracy = 99.83%: 100%|██████████| 572/572 [00:07<00:00, 81.34it/s]\n",
            "[34 / 50]   Val: Loss = 0.21466, Accuracy = 95.70%: 100%|██████████| 13/13 [00:00<00:00, 51.26it/s]\n",
            "[35 / 50] Train: Loss = 0.00463, Accuracy = 99.82%: 100%|██████████| 572/572 [00:07<00:00, 80.39it/s]\n",
            "[35 / 50]   Val: Loss = 0.21118, Accuracy = 95.79%: 100%|██████████| 13/13 [00:00<00:00, 48.54it/s]\n",
            "[36 / 50] Train: Loss = 0.00468, Accuracy = 99.82%: 100%|██████████| 572/572 [00:07<00:00, 79.33it/s]\n",
            "[36 / 50]   Val: Loss = 0.21933, Accuracy = 95.71%: 100%|██████████| 13/13 [00:00<00:00, 49.30it/s]\n",
            "[37 / 50] Train: Loss = 0.00469, Accuracy = 99.82%: 100%|██████████| 572/572 [00:07<00:00, 78.68it/s]\n",
            "[37 / 50]   Val: Loss = 0.22353, Accuracy = 95.64%: 100%|██████████| 13/13 [00:00<00:00, 50.61it/s]\n",
            "[38 / 50] Train: Loss = 0.00445, Accuracy = 99.82%: 100%|██████████| 572/572 [00:07<00:00, 79.99it/s]\n",
            "[38 / 50]   Val: Loss = 0.21624, Accuracy = 95.75%: 100%|██████████| 13/13 [00:00<00:00, 49.94it/s]\n",
            "[39 / 50] Train: Loss = 0.00501, Accuracy = 99.82%: 100%|██████████| 572/572 [00:07<00:00, 80.80it/s]\n",
            "[39 / 50]   Val: Loss = 0.23251, Accuracy = 95.58%: 100%|██████████| 13/13 [00:00<00:00, 50.96it/s]\n",
            "[40 / 50] Train: Loss = 0.00493, Accuracy = 99.81%: 100%|██████████| 572/572 [00:07<00:00, 79.65it/s]\n",
            "[40 / 50]   Val: Loss = 0.22603, Accuracy = 95.65%: 100%|██████████| 13/13 [00:00<00:00, 51.14it/s]\n",
            "[41 / 50] Train: Loss = 0.00402, Accuracy = 99.83%: 100%|██████████| 572/572 [00:07<00:00, 78.64it/s]\n",
            "[41 / 50]   Val: Loss = 0.22908, Accuracy = 95.67%: 100%|██████████| 13/13 [00:00<00:00, 49.43it/s]\n",
            "[42 / 50] Train: Loss = 0.00465, Accuracy = 99.83%: 100%|██████████| 572/572 [00:07<00:00, 80.19it/s]\n",
            "[42 / 50]   Val: Loss = 0.23146, Accuracy = 95.67%: 100%|██████████| 13/13 [00:00<00:00, 48.94it/s]\n",
            "[43 / 50] Train: Loss = 0.00540, Accuracy = 99.80%: 100%|██████████| 572/572 [00:07<00:00, 80.51it/s]\n",
            "[43 / 50]   Val: Loss = 0.22600, Accuracy = 95.67%: 100%|██████████| 13/13 [00:00<00:00, 49.98it/s]\n",
            "[44 / 50] Train: Loss = 0.00415, Accuracy = 99.83%: 100%|██████████| 572/572 [00:07<00:00, 80.12it/s]\n",
            "[44 / 50]   Val: Loss = 0.22505, Accuracy = 95.74%: 100%|██████████| 13/13 [00:00<00:00, 50.16it/s]\n",
            "[45 / 50] Train: Loss = 0.00434, Accuracy = 99.84%: 100%|██████████| 572/572 [00:07<00:00, 79.41it/s]\n",
            "[45 / 50]   Val: Loss = 0.23092, Accuracy = 95.71%: 100%|██████████| 13/13 [00:00<00:00, 50.44it/s]\n",
            "[46 / 50] Train: Loss = 0.00462, Accuracy = 99.82%: 100%|██████████| 572/572 [00:07<00:00, 80.55it/s]\n",
            "[46 / 50]   Val: Loss = 0.22535, Accuracy = 95.83%: 100%|██████████| 13/13 [00:00<00:00, 50.94it/s]\n",
            "[47 / 50] Train: Loss = 0.00460, Accuracy = 99.83%: 100%|██████████| 572/572 [00:07<00:00, 80.12it/s]\n",
            "[47 / 50]   Val: Loss = 0.22635, Accuracy = 95.76%: 100%|██████████| 13/13 [00:00<00:00, 49.08it/s]\n",
            "[48 / 50] Train: Loss = 0.00414, Accuracy = 99.84%: 100%|██████████| 572/572 [00:07<00:00, 79.53it/s]\n",
            "[48 / 50]   Val: Loss = 0.24186, Accuracy = 95.66%: 100%|██████████| 13/13 [00:00<00:00, 49.45it/s]\n",
            "[49 / 50] Train: Loss = 0.00449, Accuracy = 99.84%: 100%|██████████| 572/572 [00:07<00:00, 79.53it/s]\n",
            "[49 / 50]   Val: Loss = 0.24029, Accuracy = 95.70%: 100%|██████████| 13/13 [00:00<00:00, 49.86it/s]\n",
            "[50 / 50] Train: Loss = 0.00513, Accuracy = 99.81%: 100%|██████████| 572/572 [00:07<00:00, 79.69it/s]\n",
            "[50 / 50]   Val: Loss = 0.22696, Accuracy = 95.74%: 100%|██████████| 13/13 [00:00<00:00, 49.98it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-jxUnCxF3Fb",
        "colab_type": "code",
        "outputId": "a1f9ccb3-c441-4ea5-87ea-55f6b41a80dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "w2v_model = api.load('glove-wiki-gigaword-100')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:410: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVqfDpJWNl8I",
        "colab_type": "code",
        "outputId": "c15290d9-e378-425d-d976-9df7a5349202",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "known_count = 0\n",
        "embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n",
        "for word, ind in word2ind.items():\n",
        "    word = word.lower()\n",
        "    if word in w2v_model.vocab:\n",
        "        embeddings[ind] = w2v_model.get_vector(word)\n",
        "        known_count += 1\n",
        "        \n",
        "print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Know 38736 out of 45441 word embeddings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7SrQPL2No6Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
        "    def __init__(self, embeddings, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        self.word_embeddings = nn.Embedding.from_pretrained(embeddings)\n",
        "        self.lstm = nn.LSTM(embeddings.shape[1], lstm_hidden_dim, num_layers=lstm_layers_count, bidirectional=True)\n",
        "        self.hidden_to_tag = nn.Linear(lstm_hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeddings = self.word_embeddings(inputs)\n",
        "        lstm_out, _ = self.lstm(embeddings)\n",
        "        tag_space = self.hidden_to_tag(lstm_out)\n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        return tag_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVUDBMOpOBft",
        "colab_type": "code",
        "outputId": "7318764b-8a4f-4e24-ab20-a270ba2922e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "for X_batch, y_batch in iterate_batches((X_test, y_test), 64):\n",
        "    X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "    logits = model(X_batch)\n",
        "    mask = (y_batch != 0).float()\n",
        "    preds = torch.argmax(logits, dim=-1)\n",
        "    correct += ((preds == y_batch).float() * mask).sum()\n",
        "    total += mask.sum()\n",
        "\n",
        "print(correct/total)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.9571, device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}