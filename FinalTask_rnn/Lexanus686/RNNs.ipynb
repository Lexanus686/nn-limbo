{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinalTask.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGh_PklSqeUT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip3 -qq install torch\n",
        "#!pip3 -qq install bokeh==0.13.0\n",
        "#!pip3 -qq install gensim==3.6.0\n",
        "#!pip3 -qq install nltk\n",
        "#!pip3 -qq install scikit-learn==0.20.2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yx9jxSocrZa2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    from torch.cuda import FloatTensor, LongTensor\n",
        "else:\n",
        "    from torch import FloatTensor, LongTensor\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6xQD1sA6VEo",
        "colab_type": "code",
        "outputId": "87fdb3db-23db-488b-869d-df47bcc52fcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjje9vUyswE8",
        "colab_type": "code",
        "outputId": "af2e0dba-78d3-4afb-bc2f-7e1e54d86b73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDd4kCPHs3MV",
        "colab_type": "code",
        "outputId": "52c83603-3317-4e43-83a3-52929122b142",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "for word, tag in data[0]:\n",
        "    print('{:15}\\t{}'.format(word, tag))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The            \tDET\n",
            "Fulton         \tNOUN\n",
            "County         \tNOUN\n",
            "Grand          \tADJ\n",
            "Jury           \tNOUN\n",
            "said           \tVERB\n",
            "Friday         \tNOUN\n",
            "an             \tDET\n",
            "investigation  \tNOUN\n",
            "of             \tADP\n",
            "Atlanta's      \tNOUN\n",
            "recent         \tADJ\n",
            "primary        \tNOUN\n",
            "election       \tNOUN\n",
            "produced       \tVERB\n",
            "``             \t.\n",
            "no             \tDET\n",
            "evidence       \tNOUN\n",
            "''             \t.\n",
            "that           \tADP\n",
            "any            \tDET\n",
            "irregularities \tNOUN\n",
            "took           \tVERB\n",
            "place          \tNOUN\n",
            ".              \t.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5eGVR2qs7KZ",
        "colab_type": "code",
        "outputId": "d997632d-44f2-4429-e9fa-d58808b81c96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
        "\n",
        "print('Words count in train set:', sum(len(sent) for sent in train_data))\n",
        "print('Words count in val set:', sum(len(sent) for sent in val_data))\n",
        "print('Words count in test set:', sum(len(sent) for sent in test_data))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words count in train set: 739769\n",
            "Words count in val set: 130954\n",
            "Words count in test set: 290469\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXafdv-6uUUN",
        "colab_type": "code",
        "outputId": "86c41e57-3c7e-4c80-9320-0655c1f4bb29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "words = {word for sample in train_data for word, tag in sample}\n",
        "word2ind = {word: ind + 1 for ind, word in enumerate(words)}\n",
        "word2ind['<pad>'] = 0\n",
        "\n",
        "tags = {tag for sample in train_data for word, tag in sample}\n",
        "tag2ind = {tag: ind + 1 for ind, tag in enumerate(tags)}\n",
        "tag2ind['<pad>'] = 0\n",
        "\n",
        "print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique words in train = 45441. Tags = {'VERB', 'PRON', '.', 'PRT', 'NUM', 'ADV', 'ADP', 'CONJ', 'DET', 'NOUN', 'X', 'ADJ'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YipHcZLzu3t0",
        "colab_type": "code",
        "outputId": "d160f9c3-870f-4e0f-bb19-f82fffee754c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n",
        "tag_distribution = [tag_distribution[tag] for tag in tags]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "bar_width = 0.35\n",
        "plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n",
        "plt.xticks(np.arange(len(tags)), tags)\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmkAAAEvCAYAAAAemFY+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJElShwxpkiRJHTKkSZIkdej/AytmhLXLb1/IAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LND23RCvSyG",
        "colab_type": "code",
        "outputId": "f0aa9689-cea0-461f-e7e7-9284781e2344",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import nltk\n",
        "\n",
        "default_tagger = nltk.DefaultTagger('NN')\n",
        "\n",
        "unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n",
        "print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of unigram tagger = 92.62%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yetEASsvYS4",
        "colab_type": "code",
        "outputId": "761c3c3f-552a-4b68-e8e0-84d25e701fb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n",
        "print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of bigram tagger = 93.42%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMWR5SdTvbBT",
        "colab_type": "code",
        "outputId": "1167ebdf-05c1-4586-89c3-edb35a480aa8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "trigram_tagger = nltk.TrigramTagger(train_data)\n",
        "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of trigram tagger = 23.33%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsVkdFkavhKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_data(data, word2ind, tag2ind):\n",
        "    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n",
        "    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n",
        "X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n",
        "X_test, y_test = convert_data(test_data, word2ind, tag2ind)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQYYopZJv6_P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def iterate_batches(data, batch_size):\n",
        "    X, y = data\n",
        "    n_samples = len(X)\n",
        "\n",
        "    indices = np.arange(n_samples)\n",
        "    np.random.shuffle(indices)\n",
        "    \n",
        "    for start in range(0, n_samples, batch_size):\n",
        "        end = min(start + batch_size, n_samples)\n",
        "        \n",
        "        batch_indices = indices[start:end]\n",
        "        \n",
        "        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n",
        "        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        \n",
        "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
        "            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n",
        "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
        "            \n",
        "        yield X_batch, y_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMA6NuWRv9Og",
        "colab_type": "code",
        "outputId": "2f94dcfb-f2ce-4e02-86ae-ac5d01bc7cfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n",
        "\n",
        "X_batch.shape, y_batch.shape"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((32, 4), (32, 4))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrUGe-giv_o0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "        super(LSTMTagger, self).__init__()\n",
        "\n",
        "        self.lstm_hidden_dim = lstm_hidden_dim\n",
        "\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, word_emb_dim)\n",
        "\n",
        "        self.lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, num_layers=lstm_layers_count)\n",
        "        self.hidden2tag = nn.Linear(lstm_hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.word_embeddings(inputs)\n",
        "        lstm_out, _ = self.lstm(embeds)\n",
        "        tag_space = self.hidden2tag(lstm_out)\n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        return tag_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rxm6U-K6zvmu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ")\n",
        "\n",
        "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
        "\n",
        "logits = model(X_batch)\n",
        "\n",
        "def calc_accuracy(logits, y_batch):\n",
        "    outs = torch.argmax(logits, dim=-1)\n",
        "    reals = ((outs == y_batch).float() * (y_batch != 0).float()).sum().item()\n",
        "    totals = (y_batch != 0).float().sum().item()\n",
        "    return reals, totals"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIBfxndAzxET",
        "colab_type": "code",
        "outputId": "6985bcb6-2088-4732-9bb8-e3d2458eb275",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion(logits.view(-1, logits.shape[-1]), y_batch.view(-1)).item()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.566323757171631"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWHkPNvg4dUk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
        "    epoch_loss = 0\n",
        "    correct_count = 0\n",
        "    sum_count = 0\n",
        "    \n",
        "    is_train = not optimizer is None\n",
        "    name = name or ''\n",
        "    model.train(is_train)\n",
        "    \n",
        "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
        "    \n",
        "    with torch.autograd.set_grad_enabled(is_train):\n",
        "        with tqdm(total=batches_count) as progress_bar:\n",
        "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
        "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "                logits = model(X_batch)\n",
        "\n",
        "                loss = criterion(logits.view(-1, logits.shape[-1]), y_batch.view(-1))\n",
        "                \n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                if optimizer:\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                cur_correct_count, cur_sum_count = calc_accuracy(logits, y_batch)\n",
        "\n",
        "                correct_count += cur_correct_count\n",
        "                sum_count += cur_sum_count\n",
        "\n",
        "                progress_bar.update()\n",
        "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
        "                )\n",
        "                \n",
        "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
        "            )\n",
        "\n",
        "    return epoch_loss / batches_count, correct_count / sum_count\n",
        "\n",
        "\n",
        "def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n",
        "        val_data=None, val_batch_size=None):\n",
        "        \n",
        "    if not val_data is None and val_batch_size is None:\n",
        "        val_batch_size = batch_size\n",
        "        \n",
        "    for epoch in range(epochs_count):\n",
        "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
        "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
        "        \n",
        "        if not val_data is None:\n",
        "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPzLCjnn4yj-",
        "colab_type": "code",
        "outputId": "32d66bb6-0e75-4b2a-ef98-c4ffe363f4dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 50] Train: Loss = 0.46072, Accuracy = 68.99%: 100%|██████████| 572/572 [00:06<00:00, 92.06it/s]\n",
            "[1 / 50]   Val: Loss = 0.16410, Accuracy = 81.84%: 100%|██████████| 13/13 [00:00<00:00, 75.16it/s]\n",
            "[2 / 50] Train: Loss = 0.16459, Accuracy = 84.74%: 100%|██████████| 572/572 [00:06<00:00, 95.20it/s]\n",
            "[2 / 50]   Val: Loss = 0.12714, Accuracy = 86.73%: 100%|██████████| 13/13 [00:00<00:00, 75.82it/s]\n",
            "[3 / 50] Train: Loss = 0.12626, Accuracy = 88.08%: 100%|██████████| 572/572 [00:05<00:00, 97.47it/s] \n",
            "[3 / 50]   Val: Loss = 0.12375, Accuracy = 88.54%: 100%|██████████| 13/13 [00:00<00:00, 76.93it/s]\n",
            "[4 / 50] Train: Loss = 0.10596, Accuracy = 89.83%: 100%|██████████| 572/572 [00:05<00:00, 96.68it/s]\n",
            "[4 / 50]   Val: Loss = 0.13150, Accuracy = 89.77%: 100%|██████████| 13/13 [00:00<00:00, 77.61it/s]\n",
            "[5 / 50] Train: Loss = 0.09389, Accuracy = 90.98%: 100%|██████████| 572/572 [00:05<00:00, 95.54it/s]\n",
            "[5 / 50]   Val: Loss = 0.13885, Accuracy = 90.39%: 100%|██████████| 13/13 [00:00<00:00, 72.10it/s]\n",
            "[6 / 50] Train: Loss = 0.08508, Accuracy = 91.65%: 100%|██████████| 572/572 [00:05<00:00, 96.58it/s]\n",
            "[6 / 50]   Val: Loss = 0.13174, Accuracy = 90.94%: 100%|██████████| 13/13 [00:00<00:00, 73.71it/s]\n",
            "[7 / 50] Train: Loss = 0.07894, Accuracy = 92.19%: 100%|██████████| 572/572 [00:05<00:00, 99.17it/s]\n",
            "[7 / 50]   Val: Loss = 0.14117, Accuracy = 91.17%: 100%|██████████| 13/13 [00:00<00:00, 74.19it/s]\n",
            "[8 / 50] Train: Loss = 0.07322, Accuracy = 92.59%: 100%|██████████| 572/572 [00:05<00:00, 98.55it/s]\n",
            "[8 / 50]   Val: Loss = 0.15212, Accuracy = 91.20%: 100%|██████████| 13/13 [00:00<00:00, 73.27it/s]\n",
            "[9 / 50] Train: Loss = 0.06895, Accuracy = 92.91%: 100%|██████████| 572/572 [00:05<00:00, 95.64it/s]\n",
            "[9 / 50]   Val: Loss = 0.17317, Accuracy = 91.50%: 100%|██████████| 13/13 [00:00<00:00, 77.10it/s]\n",
            "[10 / 50] Train: Loss = 0.06594, Accuracy = 93.16%: 100%|██████████| 572/572 [00:05<00:00, 95.65it/s]\n",
            "[10 / 50]   Val: Loss = 0.19313, Accuracy = 91.54%: 100%|██████████| 13/13 [00:00<00:00, 81.29it/s]\n",
            "[11 / 50] Train: Loss = 0.06303, Accuracy = 93.37%: 100%|██████████| 572/572 [00:05<00:00, 96.89it/s] \n",
            "[11 / 50]   Val: Loss = 0.16201, Accuracy = 91.77%: 100%|██████████| 13/13 [00:00<00:00, 74.64it/s]\n",
            "[12 / 50] Train: Loss = 0.06091, Accuracy = 93.63%: 100%|██████████| 572/572 [00:05<00:00, 99.71it/s]\n",
            "[12 / 50]   Val: Loss = 0.20209, Accuracy = 91.70%: 100%|██████████| 13/13 [00:00<00:00, 77.29it/s]\n",
            "[13 / 50] Train: Loss = 0.05871, Accuracy = 93.81%: 100%|██████████| 572/572 [00:05<00:00, 98.89it/s] \n",
            "[13 / 50]   Val: Loss = 0.19118, Accuracy = 91.68%: 100%|██████████| 13/13 [00:00<00:00, 74.20it/s]\n",
            "[14 / 50] Train: Loss = 0.05717, Accuracy = 93.90%: 100%|██████████| 572/572 [00:05<00:00, 96.64it/s]\n",
            "[14 / 50]   Val: Loss = 0.19874, Accuracy = 91.76%: 100%|██████████| 13/13 [00:00<00:00, 70.84it/s]\n",
            "[15 / 50] Train: Loss = 0.05569, Accuracy = 93.99%: 100%|██████████| 572/572 [00:05<00:00, 96.91it/s]\n",
            "[15 / 50]   Val: Loss = 0.22825, Accuracy = 91.80%: 100%|██████████| 13/13 [00:00<00:00, 72.31it/s]\n",
            "[16 / 50] Train: Loss = 0.05444, Accuracy = 94.09%: 100%|██████████| 572/572 [00:05<00:00, 96.55it/s]\n",
            "[16 / 50]   Val: Loss = 0.21709, Accuracy = 91.98%: 100%|██████████| 13/13 [00:00<00:00, 71.80it/s]\n",
            "[17 / 50] Train: Loss = 0.05332, Accuracy = 94.22%: 100%|██████████| 572/572 [00:05<00:00, 99.28it/s] \n",
            "[17 / 50]   Val: Loss = 0.22698, Accuracy = 92.02%: 100%|██████████| 13/13 [00:00<00:00, 77.81it/s]\n",
            "[18 / 50] Train: Loss = 0.05227, Accuracy = 94.23%: 100%|██████████| 572/572 [00:05<00:00, 99.52it/s]\n",
            "[18 / 50]   Val: Loss = 0.26327, Accuracy = 92.06%: 100%|██████████| 13/13 [00:00<00:00, 77.58it/s]\n",
            "[19 / 50] Train: Loss = 0.05154, Accuracy = 94.33%: 100%|██████████| 572/572 [00:05<00:00, 97.18it/s]\n",
            "[19 / 50]   Val: Loss = 0.24659, Accuracy = 91.97%: 100%|██████████| 13/13 [00:00<00:00, 71.36it/s]\n",
            "[20 / 50] Train: Loss = 0.05044, Accuracy = 94.41%: 100%|██████████| 572/572 [00:05<00:00, 96.57it/s]\n",
            "[20 / 50]   Val: Loss = 0.27620, Accuracy = 91.82%: 100%|██████████| 13/13 [00:00<00:00, 76.46it/s]\n",
            "[21 / 50] Train: Loss = 0.04959, Accuracy = 94.46%: 100%|██████████| 572/572 [00:05<00:00, 96.52it/s]\n",
            "[21 / 50]   Val: Loss = 0.25947, Accuracy = 91.98%: 100%|██████████| 13/13 [00:00<00:00, 74.82it/s]\n",
            "[22 / 50] Train: Loss = 0.04909, Accuracy = 94.50%: 100%|██████████| 572/572 [00:05<00:00, 100.17it/s]\n",
            "[22 / 50]   Val: Loss = 0.28701, Accuracy = 92.02%: 100%|██████████| 13/13 [00:00<00:00, 77.34it/s]\n",
            "[23 / 50] Train: Loss = 0.04820, Accuracy = 94.54%: 100%|██████████| 572/572 [00:05<00:00, 100.40it/s]\n",
            "[23 / 50]   Val: Loss = 0.27208, Accuracy = 92.13%: 100%|██████████| 13/13 [00:00<00:00, 74.85it/s]\n",
            "[24 / 50] Train: Loss = 0.04770, Accuracy = 94.57%: 100%|██████████| 572/572 [00:05<00:00, 96.70it/s] \n",
            "[24 / 50]   Val: Loss = 0.31233, Accuracy = 92.04%: 100%|██████████| 13/13 [00:00<00:00, 76.80it/s]\n",
            "[25 / 50] Train: Loss = 0.04732, Accuracy = 94.67%: 100%|██████████| 572/572 [00:05<00:00, 96.36it/s]\n",
            "[25 / 50]   Val: Loss = 0.28319, Accuracy = 92.05%: 100%|██████████| 13/13 [00:00<00:00, 77.26it/s]\n",
            "[26 / 50] Train: Loss = 0.04646, Accuracy = 94.71%: 100%|██████████| 572/572 [00:05<00:00, 96.75it/s]\n",
            "[26 / 50]   Val: Loss = 0.28766, Accuracy = 92.13%: 100%|██████████| 13/13 [00:00<00:00, 74.70it/s]\n",
            "[27 / 50] Train: Loss = 0.04604, Accuracy = 94.78%: 100%|██████████| 572/572 [00:05<00:00, 97.13it/s] \n",
            "[27 / 50]   Val: Loss = 0.31174, Accuracy = 92.06%: 100%|██████████| 13/13 [00:00<00:00, 79.26it/s]\n",
            "[28 / 50] Train: Loss = 0.04577, Accuracy = 94.79%: 100%|██████████| 572/572 [00:05<00:00, 100.02it/s]\n",
            "[28 / 50]   Val: Loss = 0.30210, Accuracy = 92.01%: 100%|██████████| 13/13 [00:00<00:00, 73.55it/s]\n",
            "[29 / 50] Train: Loss = 0.04552, Accuracy = 94.77%: 100%|██████████| 572/572 [00:05<00:00, 98.17it/s] \n",
            "[29 / 50]   Val: Loss = 0.31430, Accuracy = 92.18%: 100%|██████████| 13/13 [00:00<00:00, 73.31it/s]\n",
            "[30 / 50] Train: Loss = 0.04511, Accuracy = 94.85%: 100%|██████████| 572/572 [00:05<00:00, 98.24it/s] \n",
            "[30 / 50]   Val: Loss = 0.32992, Accuracy = 92.05%: 100%|██████████| 13/13 [00:00<00:00, 70.21it/s]\n",
            "[31 / 50] Train: Loss = 0.04458, Accuracy = 94.85%: 100%|██████████| 572/572 [00:05<00:00, 96.11it/s]\n",
            "[31 / 50]   Val: Loss = 0.33412, Accuracy = 92.11%: 100%|██████████| 13/13 [00:00<00:00, 77.15it/s]\n",
            "[32 / 50] Train: Loss = 0.04456, Accuracy = 94.90%: 100%|██████████| 572/572 [00:05<00:00, 98.84it/s]\n",
            "[32 / 50]   Val: Loss = 0.32196, Accuracy = 92.17%: 100%|██████████| 13/13 [00:00<00:00, 76.26it/s]\n",
            "[33 / 50] Train: Loss = 0.04417, Accuracy = 94.96%: 100%|██████████| 572/572 [00:05<00:00, 99.16it/s] \n",
            "[33 / 50]   Val: Loss = 0.33993, Accuracy = 92.13%: 100%|██████████| 13/13 [00:00<00:00, 76.70it/s]\n",
            "[34 / 50] Train: Loss = 0.04384, Accuracy = 94.94%: 100%|██████████| 572/572 [00:05<00:00, 97.52it/s]\n",
            "[34 / 50]   Val: Loss = 0.35437, Accuracy = 92.17%: 100%|██████████| 13/13 [00:00<00:00, 79.15it/s]\n",
            "[35 / 50] Train: Loss = 0.04355, Accuracy = 94.95%: 100%|██████████| 572/572 [00:05<00:00, 95.77it/s]\n",
            "[35 / 50]   Val: Loss = 0.32557, Accuracy = 92.11%: 100%|██████████| 13/13 [00:00<00:00, 70.22it/s]\n",
            "[36 / 50] Train: Loss = 0.04341, Accuracy = 94.95%: 100%|██████████| 572/572 [00:05<00:00, 97.31it/s] \n",
            "[36 / 50]   Val: Loss = 0.37069, Accuracy = 92.10%: 100%|██████████| 13/13 [00:00<00:00, 70.31it/s]\n",
            "[37 / 50] Train: Loss = 0.04311, Accuracy = 95.01%: 100%|██████████| 572/572 [00:05<00:00, 98.75it/s] \n",
            "[37 / 50]   Val: Loss = 0.36276, Accuracy = 92.19%: 100%|██████████| 13/13 [00:00<00:00, 80.06it/s]\n",
            "[38 / 50] Train: Loss = 0.04287, Accuracy = 95.05%: 100%|██████████| 572/572 [00:05<00:00, 99.56it/s] \n",
            "[38 / 50]   Val: Loss = 0.33434, Accuracy = 92.03%: 100%|██████████| 13/13 [00:00<00:00, 73.50it/s]\n",
            "[39 / 50] Train: Loss = 0.04271, Accuracy = 95.03%: 100%|██████████| 572/572 [00:06<00:00, 93.93it/s]\n",
            "[39 / 50]   Val: Loss = 0.39205, Accuracy = 92.33%: 100%|██████████| 13/13 [00:00<00:00, 75.39it/s]\n",
            "[40 / 50] Train: Loss = 0.04257, Accuracy = 95.06%: 100%|██████████| 572/572 [00:05<00:00, 97.51it/s]\n",
            "[40 / 50]   Val: Loss = 0.37558, Accuracy = 92.09%: 100%|██████████| 13/13 [00:00<00:00, 75.71it/s]\n",
            "[41 / 50] Train: Loss = 0.04259, Accuracy = 95.05%: 100%|██████████| 572/572 [00:05<00:00, 96.32it/s]\n",
            "[41 / 50]   Val: Loss = 0.36338, Accuracy = 92.03%: 100%|██████████| 13/13 [00:00<00:00, 74.79it/s]\n",
            "[42 / 50] Train: Loss = 0.04243, Accuracy = 95.05%: 100%|██████████| 572/572 [00:05<00:00, 100.16it/s]\n",
            "[42 / 50]   Val: Loss = 0.40801, Accuracy = 92.19%: 100%|██████████| 13/13 [00:00<00:00, 83.27it/s]\n",
            "[43 / 50] Train: Loss = 0.04182, Accuracy = 95.10%: 100%|██████████| 572/572 [00:05<00:00, 100.01it/s]\n",
            "[43 / 50]   Val: Loss = 0.38849, Accuracy = 92.19%: 100%|██████████| 13/13 [00:00<00:00, 75.83it/s]\n",
            "[44 / 50] Train: Loss = 0.04165, Accuracy = 95.12%: 100%|██████████| 572/572 [00:05<00:00, 97.22it/s]\n",
            "[44 / 50]   Val: Loss = 0.38775, Accuracy = 92.19%: 100%|██████████| 13/13 [00:00<00:00, 74.85it/s]\n",
            "[45 / 50] Train: Loss = 0.04164, Accuracy = 95.10%: 100%|██████████| 572/572 [00:05<00:00, 95.36it/s]\n",
            "[45 / 50]   Val: Loss = 0.38724, Accuracy = 92.08%: 100%|██████████| 13/13 [00:00<00:00, 72.50it/s]\n",
            "[46 / 50] Train: Loss = 0.04143, Accuracy = 95.13%: 100%|██████████| 572/572 [00:05<00:00, 99.25it/s]\n",
            "[46 / 50]   Val: Loss = 0.39710, Accuracy = 92.08%: 100%|██████████| 13/13 [00:00<00:00, 73.80it/s]\n",
            "[47 / 50] Train: Loss = 0.04133, Accuracy = 95.17%: 100%|██████████| 572/572 [00:05<00:00, 99.52it/s]\n",
            "[47 / 50]   Val: Loss = 0.38618, Accuracy = 92.06%: 100%|██████████| 13/13 [00:00<00:00, 76.51it/s]\n",
            "[48 / 50] Train: Loss = 0.04144, Accuracy = 95.11%: 100%|██████████| 572/572 [00:05<00:00, 98.39it/s]\n",
            "[48 / 50]   Val: Loss = 0.41625, Accuracy = 92.16%: 100%|██████████| 13/13 [00:00<00:00, 79.70it/s]\n",
            "[49 / 50] Train: Loss = 0.04086, Accuracy = 95.14%: 100%|██████████| 572/572 [00:06<00:00, 94.84it/s]\n",
            "[49 / 50]   Val: Loss = 0.41205, Accuracy = 92.12%: 100%|██████████| 13/13 [00:00<00:00, 76.08it/s]\n",
            "[50 / 50] Train: Loss = 0.04095, Accuracy = 95.17%: 100%|██████████| 572/572 [00:05<00:00, 96.81it/s]\n",
            "[50 / 50]   Val: Loss = 0.41820, Accuracy = 92.21%: 100%|██████████| 13/13 [00:00<00:00, 76.49it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_Ob7TGtHSE3",
        "colab_type": "code",
        "outputId": "42f1aaed-8e0a-457b-f467-1cc0621a9923",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "for X_batch, y_batch in iterate_batches((X_test, y_test), 64):\n",
        "    X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "    logits = model(X_batch)\n",
        "    mask = (y_batch != 0).float()\n",
        "    preds = torch.argmax(logits, dim=-1)\n",
        "    correct += ((preds == y_batch).float() * mask).sum()\n",
        "    total += mask.sum()\n",
        "\n",
        "print(correct/total)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.8948, device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqPEbnWFDrRO",
        "colab_type": "code",
        "outputId": "18faf62f-9607-4a0f-da68-2f8a399abadd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "class BidirectionalLSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.word_embeddings = nn.Embedding(vocab_size, word_emb_dim)\n",
        "        self.lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, num_layers=lstm_layers_count, bidirectional=True)\n",
        "        self.hidden_to_tag = nn.Linear(2 * lstm_hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeddings = self.word_embeddings(inputs)\n",
        "        lstm_out, _ = self.lstm(embeddings)\n",
        "        tag_space = self.hidden_to_tag(lstm_out)\n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        return tag_scores\n",
        "\n",
        "model = BidirectionalLSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 50] Train: Loss = 0.60491, Accuracy = 81.52%: 100%|██████████| 572/572 [00:07<00:00, 80.13it/s]\n",
            "[1 / 50]   Val: Loss = 0.30553, Accuracy = 90.03%: 100%|██████████| 13/13 [00:00<00:00, 60.88it/s]\n",
            "[2 / 50] Train: Loss = 0.22916, Accuracy = 92.77%: 100%|██████████| 572/572 [00:07<00:00, 81.08it/s]\n",
            "[2 / 50]   Val: Loss = 0.20591, Accuracy = 93.30%: 100%|██████████| 13/13 [00:00<00:00, 65.59it/s]\n",
            "[3 / 50] Train: Loss = 0.15036, Accuracy = 95.37%: 100%|██████████| 572/572 [00:07<00:00, 80.89it/s]\n",
            "[3 / 50]   Val: Loss = 0.17715, Accuracy = 94.23%: 100%|██████████| 13/13 [00:00<00:00, 65.72it/s]\n",
            "[4 / 50] Train: Loss = 0.10857, Accuracy = 96.68%: 100%|██████████| 572/572 [00:07<00:00, 80.43it/s]\n",
            "[4 / 50]   Val: Loss = 0.15854, Accuracy = 94.74%: 100%|██████████| 13/13 [00:00<00:00, 65.24it/s]\n",
            "[5 / 50] Train: Loss = 0.07911, Accuracy = 97.62%: 100%|██████████| 572/572 [00:07<00:00, 80.72it/s]\n",
            "[5 / 50]   Val: Loss = 0.14173, Accuracy = 95.31%: 100%|██████████| 13/13 [00:00<00:00, 64.93it/s]\n",
            "[6 / 50] Train: Loss = 0.05788, Accuracy = 98.27%: 100%|██████████| 572/572 [00:06<00:00, 82.10it/s]\n",
            "[6 / 50]   Val: Loss = 0.14403, Accuracy = 95.33%: 100%|██████████| 13/13 [00:00<00:00, 67.70it/s]\n",
            "[7 / 50] Train: Loss = 0.04414, Accuracy = 98.72%: 100%|██████████| 572/572 [00:07<00:00, 75.79it/s]\n",
            "[7 / 50]   Val: Loss = 0.14195, Accuracy = 95.49%: 100%|██████████| 13/13 [00:00<00:00, 62.95it/s]\n",
            "[8 / 50] Train: Loss = 0.03280, Accuracy = 99.06%: 100%|██████████| 572/572 [00:07<00:00, 80.12it/s]\n",
            "[8 / 50]   Val: Loss = 0.14896, Accuracy = 95.28%: 100%|██████████| 13/13 [00:00<00:00, 66.07it/s]\n",
            "[9 / 50] Train: Loss = 0.02524, Accuracy = 99.28%: 100%|██████████| 572/572 [00:07<00:00, 81.17it/s]\n",
            "[9 / 50]   Val: Loss = 0.15654, Accuracy = 95.32%: 100%|██████████| 13/13 [00:00<00:00, 64.27it/s]\n",
            "[10 / 50] Train: Loss = 0.01881, Accuracy = 99.49%: 100%|██████████| 572/572 [00:07<00:00, 81.25it/s]\n",
            "[10 / 50]   Val: Loss = 0.16402, Accuracy = 95.25%: 100%|██████████| 13/13 [00:00<00:00, 63.30it/s]\n",
            "[11 / 50] Train: Loss = 0.01557, Accuracy = 99.58%: 100%|██████████| 572/572 [00:06<00:00, 82.26it/s]\n",
            "[11 / 50]   Val: Loss = 0.16096, Accuracy = 95.45%: 100%|██████████| 13/13 [00:00<00:00, 66.56it/s]\n",
            "[12 / 50] Train: Loss = 0.01270, Accuracy = 99.64%: 100%|██████████| 572/572 [00:06<00:00, 81.90it/s]\n",
            "[12 / 50]   Val: Loss = 0.16720, Accuracy = 95.48%: 100%|██████████| 13/13 [00:00<00:00, 60.79it/s]\n",
            "[13 / 50] Train: Loss = 0.01020, Accuracy = 99.71%: 100%|██████████| 572/572 [00:07<00:00, 79.89it/s]\n",
            "[13 / 50]   Val: Loss = 0.16928, Accuracy = 95.45%: 100%|██████████| 13/13 [00:00<00:00, 64.94it/s]\n",
            "[14 / 50] Train: Loss = 0.00951, Accuracy = 99.75%: 100%|██████████| 572/572 [00:06<00:00, 82.16it/s]\n",
            "[14 / 50]   Val: Loss = 0.20438, Accuracy = 95.10%: 100%|██████████| 13/13 [00:00<00:00, 63.00it/s]\n",
            "[15 / 50] Train: Loss = 0.01020, Accuracy = 99.69%: 100%|██████████| 572/572 [00:07<00:00, 80.40it/s]\n",
            "[15 / 50]   Val: Loss = 0.19290, Accuracy = 95.33%: 100%|██████████| 13/13 [00:00<00:00, 65.63it/s]\n",
            "[16 / 50] Train: Loss = 0.00750, Accuracy = 99.77%: 100%|██████████| 572/572 [00:07<00:00, 79.81it/s]\n",
            "[16 / 50]   Val: Loss = 0.19454, Accuracy = 95.34%: 100%|██████████| 13/13 [00:00<00:00, 64.90it/s]\n",
            "[17 / 50] Train: Loss = 0.00759, Accuracy = 99.77%: 100%|██████████| 572/572 [00:07<00:00, 79.73it/s]\n",
            "[17 / 50]   Val: Loss = 0.20095, Accuracy = 95.34%: 100%|██████████| 13/13 [00:00<00:00, 66.62it/s]\n",
            "[18 / 50] Train: Loss = 0.00724, Accuracy = 99.77%: 100%|██████████| 572/572 [00:07<00:00, 79.47it/s]\n",
            "[18 / 50]   Val: Loss = 0.20712, Accuracy = 95.39%: 100%|██████████| 13/13 [00:00<00:00, 65.58it/s]\n",
            "[19 / 50] Train: Loss = 0.00645, Accuracy = 99.79%: 100%|██████████| 572/572 [00:07<00:00, 81.02it/s]\n",
            "[19 / 50]   Val: Loss = 0.21873, Accuracy = 95.33%: 100%|██████████| 13/13 [00:00<00:00, 62.79it/s]\n",
            "[20 / 50] Train: Loss = 0.00673, Accuracy = 99.77%: 100%|██████████| 572/572 [00:07<00:00, 81.45it/s]\n",
            "[20 / 50]   Val: Loss = 0.22057, Accuracy = 95.27%: 100%|██████████| 13/13 [00:00<00:00, 63.75it/s]\n",
            "[21 / 50] Train: Loss = 0.00552, Accuracy = 99.81%: 100%|██████████| 572/572 [00:07<00:00, 79.16it/s]\n",
            "[21 / 50]   Val: Loss = 0.23605, Accuracy = 95.16%: 100%|██████████| 13/13 [00:00<00:00, 60.55it/s]\n",
            "[22 / 50] Train: Loss = 0.00605, Accuracy = 99.79%: 100%|██████████| 572/572 [00:07<00:00, 80.39it/s]\n",
            "[22 / 50]   Val: Loss = 0.23676, Accuracy = 95.24%: 100%|██████████| 13/13 [00:00<00:00, 67.18it/s]\n",
            "[23 / 50] Train: Loss = 0.00563, Accuracy = 99.80%: 100%|██████████| 572/572 [00:07<00:00, 80.99it/s]\n",
            "[23 / 50]   Val: Loss = 0.23661, Accuracy = 95.15%: 100%|██████████| 13/13 [00:00<00:00, 64.77it/s]\n",
            "[24 / 50] Train: Loss = 0.00566, Accuracy = 99.79%: 100%|██████████| 572/572 [00:07<00:00, 80.68it/s]\n",
            "[24 / 50]   Val: Loss = 0.24756, Accuracy = 95.21%: 100%|██████████| 13/13 [00:00<00:00, 64.90it/s]\n",
            "[25 / 50] Train: Loss = 0.00507, Accuracy = 99.82%: 100%|██████████| 572/572 [00:07<00:00, 79.18it/s]\n",
            "[25 / 50]   Val: Loss = 0.24367, Accuracy = 95.29%: 100%|██████████| 13/13 [00:00<00:00, 66.66it/s]\n",
            "[26 / 50] Train: Loss = 0.00532, Accuracy = 99.80%: 100%|██████████| 572/572 [00:07<00:00, 78.52it/s]\n",
            "[26 / 50]   Val: Loss = 0.23597, Accuracy = 95.28%: 100%|██████████| 13/13 [00:00<00:00, 65.91it/s]\n",
            "[27 / 50] Train: Loss = 0.00521, Accuracy = 99.81%: 100%|██████████| 572/572 [00:07<00:00, 81.68it/s]\n",
            "[27 / 50]   Val: Loss = 0.24483, Accuracy = 95.33%: 100%|██████████| 13/13 [00:00<00:00, 61.29it/s]\n",
            "[28 / 50] Train: Loss = 0.00489, Accuracy = 99.81%: 100%|██████████| 572/572 [00:07<00:00, 79.98it/s]\n",
            "[28 / 50]   Val: Loss = 0.24429, Accuracy = 95.29%: 100%|██████████| 13/13 [00:00<00:00, 64.83it/s]\n",
            "[29 / 50] Train: Loss = 0.00509, Accuracy = 99.82%: 100%|██████████| 572/572 [00:07<00:00, 79.67it/s]\n",
            "[29 / 50]   Val: Loss = 0.24386, Accuracy = 95.31%: 100%|██████████| 13/13 [00:00<00:00, 67.18it/s]\n",
            "[30 / 50] Train: Loss = 0.00554, Accuracy = 99.80%: 100%|██████████| 572/572 [00:07<00:00, 80.43it/s]\n",
            "[30 / 50]   Val: Loss = 0.25648, Accuracy = 95.26%: 100%|██████████| 13/13 [00:00<00:00, 66.76it/s]\n",
            "[31 / 50] Train: Loss = 0.00503, Accuracy = 99.81%: 100%|██████████| 572/572 [00:07<00:00, 81.09it/s]\n",
            "[31 / 50]   Val: Loss = 0.23623, Accuracy = 95.46%: 100%|██████████| 13/13 [00:00<00:00, 68.20it/s]\n",
            "[32 / 50] Train: Loss = 0.00517, Accuracy = 99.81%: 100%|██████████| 572/572 [00:07<00:00, 80.40it/s]\n",
            "[32 / 50]   Val: Loss = 0.25292, Accuracy = 95.42%: 100%|██████████| 13/13 [00:00<00:00, 54.99it/s]\n",
            "[33 / 50] Train: Loss = 0.00489, Accuracy = 99.82%: 100%|██████████| 572/572 [00:07<00:00, 79.27it/s]\n",
            "[33 / 50]   Val: Loss = 0.24883, Accuracy = 95.39%: 100%|██████████| 13/13 [00:00<00:00, 64.17it/s]\n",
            "[34 / 50] Train: Loss = 0.00475, Accuracy = 99.83%: 100%|██████████| 572/572 [00:07<00:00, 79.71it/s]\n",
            "[34 / 50]   Val: Loss = 0.25802, Accuracy = 95.35%: 100%|██████████| 13/13 [00:00<00:00, 62.29it/s]\n",
            "[35 / 50] Train: Loss = 0.00478, Accuracy = 99.82%: 100%|██████████| 572/572 [00:07<00:00, 81.36it/s]\n",
            "[35 / 50]   Val: Loss = 0.25477, Accuracy = 95.36%: 100%|██████████| 13/13 [00:00<00:00, 68.07it/s]\n",
            "[36 / 50] Train: Loss = 0.00455, Accuracy = 99.82%: 100%|██████████| 572/572 [00:06<00:00, 82.57it/s]\n",
            "[36 / 50]   Val: Loss = 0.25154, Accuracy = 95.41%: 100%|██████████| 13/13 [00:00<00:00, 60.08it/s]\n",
            "[37 / 50] Train: Loss = 0.00482, Accuracy = 99.83%: 100%|██████████| 572/572 [00:07<00:00, 80.43it/s]\n",
            "[37 / 50]   Val: Loss = 0.24799, Accuracy = 95.38%: 100%|██████████| 13/13 [00:00<00:00, 66.79it/s]\n",
            "[38 / 50] Train: Loss = 0.00453, Accuracy = 99.82%: 100%|██████████| 572/572 [00:07<00:00, 79.11it/s]\n",
            "[38 / 50]   Val: Loss = 0.25113, Accuracy = 95.34%: 100%|██████████| 13/13 [00:00<00:00, 65.11it/s]\n",
            "[39 / 50] Train: Loss = 0.00465, Accuracy = 99.83%: 100%|██████████| 572/572 [00:07<00:00, 80.61it/s]\n",
            "[39 / 50]   Val: Loss = 0.25263, Accuracy = 95.41%: 100%|██████████| 13/13 [00:00<00:00, 64.55it/s]\n",
            "[40 / 50] Train: Loss = 0.00500, Accuracy = 99.81%: 100%|██████████| 572/572 [00:07<00:00, 80.93it/s]\n",
            "[40 / 50]   Val: Loss = 0.24443, Accuracy = 95.49%: 100%|██████████| 13/13 [00:00<00:00, 64.56it/s]\n",
            "[41 / 50] Train: Loss = 0.00444, Accuracy = 99.83%: 100%|██████████| 572/572 [00:07<00:00, 79.78it/s]\n",
            "[41 / 50]   Val: Loss = 0.23912, Accuracy = 95.52%: 100%|██████████| 13/13 [00:00<00:00, 63.30it/s]\n",
            "[42 / 50] Train: Loss = 0.00495, Accuracy = 99.82%: 100%|██████████| 572/572 [00:07<00:00, 79.01it/s]\n",
            "[42 / 50]   Val: Loss = 0.23995, Accuracy = 95.61%: 100%|██████████| 13/13 [00:00<00:00, 64.33it/s]\n",
            "[43 / 50] Train: Loss = 0.00522, Accuracy = 99.80%: 100%|██████████| 572/572 [00:07<00:00, 80.67it/s]\n",
            "[43 / 50]   Val: Loss = 0.25522, Accuracy = 95.43%: 100%|██████████| 13/13 [00:00<00:00, 67.65it/s]\n",
            "[44 / 50] Train: Loss = 0.00461, Accuracy = 99.83%: 100%|██████████| 572/572 [00:06<00:00, 82.36it/s]\n",
            "[44 / 50]   Val: Loss = 0.26088, Accuracy = 95.38%: 100%|██████████| 13/13 [00:00<00:00, 63.08it/s]\n",
            "[45 / 50] Train: Loss = 0.00443, Accuracy = 99.82%: 100%|██████████| 572/572 [00:07<00:00, 79.14it/s]\n",
            "[45 / 50]   Val: Loss = 0.26605, Accuracy = 95.42%: 100%|██████████| 13/13 [00:00<00:00, 61.94it/s]\n",
            "[46 / 50] Train: Loss = 0.00387, Accuracy = 99.85%: 100%|██████████| 572/572 [00:07<00:00, 79.09it/s]\n",
            "[46 / 50]   Val: Loss = 0.26605, Accuracy = 95.38%: 100%|██████████| 13/13 [00:00<00:00, 61.46it/s]\n",
            "[47 / 50] Train: Loss = 0.00397, Accuracy = 99.85%: 100%|██████████| 572/572 [00:07<00:00, 80.31it/s]\n",
            "[47 / 50]   Val: Loss = 0.25680, Accuracy = 95.45%: 100%|██████████| 13/13 [00:00<00:00, 64.13it/s]\n",
            "[48 / 50] Train: Loss = 0.00436, Accuracy = 99.83%: 100%|██████████| 572/572 [00:07<00:00, 80.05it/s]\n",
            "[48 / 50]   Val: Loss = 0.28050, Accuracy = 95.35%: 100%|██████████| 13/13 [00:00<00:00, 66.55it/s]\n",
            "[49 / 50] Train: Loss = 0.00397, Accuracy = 99.84%: 100%|██████████| 572/572 [00:07<00:00, 78.93it/s]\n",
            "[49 / 50]   Val: Loss = 0.26506, Accuracy = 95.39%: 100%|██████████| 13/13 [00:00<00:00, 57.32it/s]\n",
            "[50 / 50] Train: Loss = 0.00391, Accuracy = 99.85%: 100%|██████████| 572/572 [00:07<00:00, 80.08it/s]\n",
            "[50 / 50]   Val: Loss = 0.26659, Accuracy = 95.44%: 100%|██████████| 13/13 [00:00<00:00, 63.96it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDk4BtokpkZf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b98d9969-08dc-4020-bc67-6ce8e5086a15"
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "for X_batch, y_batch in iterate_batches((X_test, y_test), 64):\n",
        "    X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "    logits = model(X_batch)\n",
        "    mask = (y_batch != 0).float()\n",
        "    preds = torch.argmax(logits, dim=-1)\n",
        "    correct += ((preds == y_batch).float() * mask).sum()\n",
        "    total += mask.sum()\n",
        "\n",
        "print(correct/total)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.9538, device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-jxUnCxF3Fb",
        "colab_type": "code",
        "outputId": "5ac0a845-4e63-46ed-ccad-17772327d331",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "w2v_model = api.load('glove-wiki-gigaword-100')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:410: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVqfDpJWNl8I",
        "colab_type": "code",
        "outputId": "bde7f300-04fd-4099-ae81-221c719a200e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "known_count = 0\n",
        "embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n",
        "for word, ind in word2ind.items():\n",
        "    word = word.lower()\n",
        "    if word in w2v_model.vocab:\n",
        "        embeddings[ind] = w2v_model.get_vector(word)\n",
        "        known_count += 1\n",
        "        \n",
        "print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Know 38736 out of 45441 word embeddings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7SrQPL2No6Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
        "    def __init__(self, embeddings, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        self.word_embeddings = nn.Embedding.from_pretrained(embeddings)\n",
        "        self.lstm = nn.LSTM(embeddings.shape[1], lstm_hidden_dim, num_layers=lstm_layers_count, bidirectional=True)\n",
        "        self.hidden_to_tag = nn.Linear(2*lstm_hidden_dim, tagset_size)\n",
        "   \n",
        "    def forward(self, inputs):\n",
        "        embeddings = self.word_embeddings(inputs)\n",
        "        out, _ = self.lstm(embeddings)\n",
        "        return self.hidden_to_tag(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0n3xCJ7mR-2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "febe8ba9-3254-4264-a5b4-314db132844f"
      },
      "source": [
        "model = LSTMTaggerWithPretrainedEmbs(\n",
        "    embeddings=torch.FloatTensor(embeddings),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 50] Train: Loss = 0.56668, Accuracy = 83.49%: 100%|██████████| 572/572 [00:06<00:00, 88.33it/s]\n",
            "[1 / 50]   Val: Loss = 0.25853, Accuracy = 92.39%: 100%|██████████| 13/13 [00:00<00:00, 72.93it/s]\n",
            "[2 / 50] Train: Loss = 0.18891, Accuracy = 94.39%: 100%|██████████| 572/572 [00:06<00:00, 84.60it/s]\n",
            "[2 / 50]   Val: Loss = 0.17294, Accuracy = 94.73%: 100%|██████████| 13/13 [00:00<00:00, 74.52it/s]\n",
            "[3 / 50] Train: Loss = 0.13392, Accuracy = 95.99%: 100%|██████████| 572/572 [00:06<00:00, 86.68it/s]\n",
            "[3 / 50]   Val: Loss = 0.14187, Accuracy = 95.61%: 100%|██████████| 13/13 [00:00<00:00, 74.28it/s]\n",
            "[4 / 50] Train: Loss = 0.10815, Accuracy = 96.73%: 100%|██████████| 572/572 [00:06<00:00, 85.09it/s]\n",
            "[4 / 50]   Val: Loss = 0.12329, Accuracy = 96.16%: 100%|██████████| 13/13 [00:00<00:00, 75.71it/s]\n",
            "[5 / 50] Train: Loss = 0.10477, Accuracy = 96.99%:  40%|████      | 230/572 [00:02<00:03, 87.67it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVUDBMOpOBft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "for X_batch, y_batch in iterate_batches((X_test, y_test), 64):\n",
        "    X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "    logits = model(X_batch)\n",
        "    mask = (y_batch != 0).float()\n",
        "    preds = torch.argmax(logits, dim=-1)\n",
        "    correct += ((preds == y_batch).float() * mask).sum()\n",
        "    total += mask.sum()\n",
        "\n",
        "print(correct/total)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}